{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # noqa: E402\n",
    "from pathlib import Path  # noqa: E402\n",
    "from typing import List  # noqa: E402\n",
    "\n",
    "import h5py  # noqa: E402\n",
    "import numpy as np  # noqa: E402\n",
    "import pandas as pd  # noqa: E402\n",
    "from pandarallel import pandarallel  # noqa: E402\n",
    "from rdkit import RDLogger  # noqa: E402\n",
    "from rdkit.Chem import Descriptors  # noqa: E402\n",
    "from rdkit.Chem.rdmolfiles import (  # noqa: E402\n",
    "    MolFromSmarts,\n",
    "    MolFromSmiles,\n",
    "    MolToSmiles,\n",
    "    SDMolSupplier,\n",
    ")\n",
    "from tdc.utils import create_fold, create_scaffold_split  # noqa: E402\n",
    "from tokenizers import Tokenizer  # noqa: E402\n",
    "from tqdm.auto import tqdm, trange  # noqa: E402\n",
    "\n",
    "from src.data.components.utils import (  # noqa: E402\n",
    "    create_butina_split,\n",
    "    smiles2vector_fg,\n",
    "    smiles2vector_mfg,\n",
    "    std_smarts,\n",
    "    std_smiles,\n",
    ")\n",
    "\n",
    "RDLogger.DisableLog(\"rdApp.*\")\n",
    "pandarallel.initialize(progress_bar=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(\n",
    "    task: str,\n",
    "    data_path: str,\n",
    "    smiles_col: str,\n",
    "    target_cols: List[str],\n",
    "    classification: bool,\n",
    "    write: bool,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Preprocesses a dataframe for a given task.\n",
    "\n",
    "    :param task: Task name\n",
    "    :param data_path: Data path\n",
    "    :param smiles_col: SMILES column name\n",
    "    :param target_cols: Target column names\n",
    "    :param classification: Classification task\n",
    "    :param write: Write to disk\n",
    "    :return: Preprocessed dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(data_path)  # read dataframe\n",
    "    if classification:\n",
    "        df.fillna(0, inplace=True)  # fill nulls with 0\n",
    "    if target_cols == [\"multi_target\"]:\n",
    "        target_cols = df.columns[1:].to_list()  # select all columns except the first\n",
    "    df = df.loc[:, [smiles_col] + target_cols]  # select SMILES and target columns\n",
    "    df[\"SMILES\"] = df[smiles_col].parallel_apply(std_smiles)  # standardize SMILES\n",
    "    df.drop(columns=[smiles_col], inplace=True)  # drop original SMILES column\n",
    "    df.dropna(inplace=True)  # drop nulls\n",
    "    df.drop_duplicates(subset=[\"SMILES\"], inplace=True)  # drop duplicates\n",
    "    df.reset_index(drop=True, inplace=True)  # reset index\n",
    "    if write:\n",
    "        write_path = Path(f\"./data/processed/tasks/{task}\")\n",
    "        write_path.mkdir(parents=True, exist_ok=True)  # create directory if it doesn't exist\n",
    "        df.to_parquet(write_path / f\"{task}.parquet\", index=False)  # write to disk\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_summary(data_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Creates a summary dataframe for a given task.\n",
    "\n",
    "    :param data_path: Data path\n",
    "    :return: Summary dataframe\n",
    "    \"\"\"\n",
    "    num_datapoints = []  # number of datapoints per task\n",
    "    num_tasks = []  # number of tasks per task\n",
    "    tasks = []  # task names\n",
    "    for task_path in Path(data_path).iterdir():  # iterate over tasks\n",
    "        if not task_path.is_dir():  # skip files\n",
    "            continue\n",
    "        task = task_path.name  # get task name\n",
    "        df = pd.read_parquet(task_path / f\"{task}.parquet\")  # read dataframe\n",
    "        num_datapoints.append(df.shape[0])  # append number of datapoints\n",
    "        num_tasks.append(df.shape[1] - 1)  # append number of tasks\n",
    "        tasks.append(task)  # append task name\n",
    "    df = (\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"Task\": tasks,\n",
    "                \"Datapoints\": num_datapoints,\n",
    "                \"Num_tasks\": num_tasks,\n",
    "            }\n",
    "        )\n",
    "        .sort_values(\"Task\")\n",
    "        .reset_index(drop=True)  # create summary dataframe\n",
    "    )\n",
    "    df.to_parquet(Path(data_path) / \"summary.parquet\", index=False)  # write to disk\n",
    "    return df  # return summary dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional Groups Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/raw/training/fg.csv\")  # read dataframe\n",
    "drop_values = [\n",
    "    \"NOT [!#1!#6!#7!#8!#16!#15!#9!#17!#35!#53] AND [#6] >0\",\n",
    "    \"[!#1;!#6;!#7;!#8;!#9;!#11;!#12;!#15;!#16;!#17;!#19;!#20;!#35;!#53]\",\n",
    "    \"NOT [!#6!#1!#7!#8!#9!#17!#35!#53!#15!#14!#16]\",\n",
    "    \"[!#6!#1!#7!#8!#9!#17!#35!#53!#15!#14!#16]\",\n",
    "    \"[!#1!#6!#7!#8!#16!#15!#9!#17!#35!#53]\",\n",
    "    \"C(F)(F)C(F)(F)\",\n",
    "    \"(H[Cl,Br,I,F]).([*])\",\n",
    "    \"[!#1!#2!#5!#6!#7!#8!#9!#10!#14!#15!#16!#17!#18!#32!#33!#34!#35!#36!#51!#52!#53!#54!#84!#85!#86]~[#6,#7,#8,#16]\",\n",
    "    \"([*]).([*])\",\n",
    "    \"[#6] >0\",\n",
    "    \"[N;!R][C;!R](=O)[!#7] OR [N;!R][C;!R](=O)[N;!R]\",\n",
    "    \"[#16;X3v3+0]\",\n",
    "    \"N[F,Cl,Br,I]\",\n",
    "    \"[+,-]\",\n",
    "    \"([*][*][*]).([*][*][*])\",\n",
    "    \"[+]\",\n",
    "    \"[-]\",\n",
    "    \"[ERROR]\",\n",
    "]  # SMARTS to drop\n",
    "df = df[~df[\"SMARTS\"].isin(drop_values)].reset_index(drop=True)  # drop SMARTS\n",
    "df[\"SMARTS\"] = df[\"SMARTS\"].parallel_apply(std_smarts)  # standardize SMARTS\n",
    "df.dropna(inplace=True)  # drop nulls\n",
    "df.drop_duplicates(subset=[\"SMARTS\"], inplace=True)  # drop duplicates\n",
    "df.to_parquet(\"./data/processed/training/fg.parquet\", index=False)  # write to disk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Therapeutic Data Commons Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_tdc = [\n",
    "    \"herg_karim\",\n",
    "    \"herg\",\n",
    "    \"herg_central_inhib\",\n",
    "    \"dili\",\n",
    "    \"skin_reaction\",\n",
    "    \"ames\",\n",
    "    \"carcinogens_lagunin\",\n",
    "    \"sarscov2_3clpro_diamond\",\n",
    "    \"sarscov2_vitro_touret\",\n",
    "    \"orexin1_receptor_butkiewicz\",\n",
    "    \"m1_muscarinic_receptor_agonists_butkiewicz\",\n",
    "    \"m1_muscarinic_receptor_antagonists_butkiewicz\",\n",
    "    \"potassium_ion_channel_kir2.1_butkiewicz\",\n",
    "    \"kcnq2_potassium_channel_butkiewicz\",\n",
    "    \"cav3_t-type_calcium_channels_butkiewicz\",\n",
    "    \"choline_transporter_butkiewicz\",\n",
    "    \"serine_threonine_kinase_33_butkiewicz\",\n",
    "    \"tyrosyl-dna_phosphodiesterase_butkiewicz\",\n",
    "    \"pampa_ncats\",\n",
    "    \"hia_hou\",\n",
    "    \"pgp_broccatelli\",\n",
    "    \"bioavailability_ma\",\n",
    "    \"cyp2c9_substrate_carbonmangels\",\n",
    "    \"cyp2d6_substrate_carbonmangels\",\n",
    "    \"cyp3a4_substrate_carbonmangels\",\n",
    "]\n",
    "\n",
    "regression_tdc = [\n",
    "    \"ld50_zhu\",\n",
    "    \"herg_central_1uM\",\n",
    "    \"herg_central_10uM\",\n",
    "    \"caco2_wang\",\n",
    "    \"solubility_aqsoldb\",\n",
    "    \"ppbr_az\",\n",
    "    \"vdss_lombardo\",\n",
    "    \"half_life_obach\",\n",
    "    \"clearance_hepatocyte_az\",\n",
    "    \"clearance_microsome_az\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in tqdm(classification_tdc, desc=\"Processing TDC Classification datasets\"):\n",
    "    _ = preprocess_df(\n",
    "        task=task,\n",
    "        data_path=f\"./data/raw/tdc_data/{task}.csv.gz\",\n",
    "        smiles_col=\"Drug\",\n",
    "        target_cols=[\"Y\"],\n",
    "        classification=True,\n",
    "        write=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in tqdm(regression_tdc, desc=\"Processing TDC Regression datasets\"):\n",
    "    _ = preprocess_df(\n",
    "        task=task,\n",
    "        data_path=f\"./data/raw/tdc_data/{task}.csv.gz\",\n",
    "        smiles_col=\"Drug\",\n",
    "        target_cols=[\"Y\"],\n",
    "        classification=False,\n",
    "        write=True,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MoleculeNet Prep"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_tasks = {\n",
    "    \"Lipop\": [\"exp\"],\n",
    "    \"ESOL\": [\"measured log solubility in mols per litre\"],\n",
    "    \"FreeSolv\": [\"expt\"],\n",
    "    \"PDBbind-full\": [\"-logKd/Ki\"],\n",
    "    \"PDBbind-core\": [\"-logKd/Ki\"],\n",
    "    \"PDBbind-refined\": [\"-logKd/Ki\"],\n",
    "    \"qm7\": [\"u0_atom\"],\n",
    "    \"qm8\": [\n",
    "        \"E1-CC2\",\n",
    "        \"E2-CC2\",\n",
    "        \"f1-CC2\",\n",
    "        \"f2-CC2\",\n",
    "        \"E1-PBE0\",\n",
    "        \"E2-PBE0\",\n",
    "        \"f1-PBE0\",\n",
    "        \"f2-PBE0\",\n",
    "        \"E1-CAM\",\n",
    "        \"E2-CAM\",\n",
    "        \"f1-CAM\",\n",
    "        \"f2-CAM\",\n",
    "    ],\n",
    "    \"qm9\": [\n",
    "        \"mu\",\n",
    "        \"alpha\",\n",
    "        \"homo\",\n",
    "        \"lumo\",\n",
    "        \"gap\",\n",
    "        \"r2\",\n",
    "        \"zpve\",\n",
    "        \"cv\",\n",
    "        \"u0\",\n",
    "        \"u298\",\n",
    "        \"h298\",\n",
    "        \"g298\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in tqdm(regression_tasks.keys(), desc=\"Processing Regression Tasks\"):\n",
    "    _ = preprocess_df(\n",
    "        task=task,\n",
    "        data_path=f\"./data/raw/moleculenet/{task}/{task}.csv.gz\",\n",
    "        smiles_col=\"smiles\",\n",
    "        target_cols=regression_tasks[task],\n",
    "        classification=False,\n",
    "        write=True,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_tasks = {\n",
    "    \"ToxCast\": [\"multi_target\"],\n",
    "    \"BACE\": [\"Class\"],\n",
    "    \"ChEMBL\": [\"multi_target\"],\n",
    "    \"PCBA\": [\"multi_target\"],\n",
    "    \"BBBP\": [\"p_np\"],\n",
    "    \"HIV\": [\"HIV_active\"],\n",
    "    \"MUV\": [\n",
    "        \"MUV-466\",\n",
    "        \"MUV-548\",\n",
    "        \"MUV-600\",\n",
    "        \"MUV-644\",\n",
    "        \"MUV-652\",\n",
    "        \"MUV-689\",\n",
    "        \"MUV-692\",\n",
    "        \"MUV-712\",\n",
    "        \"MUV-713\",\n",
    "        \"MUV-733\",\n",
    "        \"MUV-737\",\n",
    "        \"MUV-810\",\n",
    "        \"MUV-832\",\n",
    "        \"MUV-846\",\n",
    "        \"MUV-852\",\n",
    "        \"MUV-858\",\n",
    "        \"MUV-859\",\n",
    "    ],\n",
    "    \"Tox21\": [\n",
    "        \"NR-AR\",\n",
    "        \"NR-AR-LBD\",\n",
    "        \"NR-AhR\",\n",
    "        \"NR-Aromatase\",\n",
    "        \"NR-ER\",\n",
    "        \"NR-ER-LBD\",\n",
    "        \"NR-PPAR-gamma\",\n",
    "        \"SR-ARE\",\n",
    "        \"SR-ATAD5\",\n",
    "        \"SR-HSE\",\n",
    "        \"SR-MMP\",\n",
    "        \"SR-p53\",\n",
    "    ],\n",
    "    \"SIDER\": [\"multi_target\"],\n",
    "    \"ClinTox\": [\"FDA_APPROVED\", \"CT_TOX\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in tqdm(classification_tasks.keys(), desc=\"Processing Classification Tasks\"):\n",
    "    _ = preprocess_df(\n",
    "        task=task,\n",
    "        data_path=f\"./data/raw/moleculenet/{task}/{task}.csv.gz\",\n",
    "        smiles_col=\"smiles\",\n",
    "        target_cols=classification_tasks[task],\n",
    "        classification=True,\n",
    "        write=True,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peptide Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peptide_tasks = {\n",
    "    \"1625_aa\": {\"smiles_col\": \"Peptide_smiles\", \"target_cols\": [\"cleavage\"]},\n",
    "    \"746_aa\": {\"smiles_col\": \"Peptide_smiles\", \"target_cols\": [\"cleavage\"]},\n",
    "    \"Impens\": {\"smiles_col\": \"Peptide_smiles\", \"target_cols\": [\"cleavage\"]},\n",
    "    \"Schilling\": {\"smiles_col\": \"Peptide_smiles\", \"target_cols\": [\"cleavage\"]},\n",
    "    \"Ecoli\": {\"smiles_col\": \"smiles\", \"target_cols\": [\"Encoded_Activity\"]},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in tqdm(peptide_tasks.keys(), desc=\"Processing Peptide Tasks\"):\n",
    "    _ = preprocess_df(\n",
    "        task=task,\n",
    "        data_path=f\"./data/raw/peptide_data/{task}.csv.gz\",\n",
    "        smiles_col=peptide_tasks[task][\"smiles_col\"],\n",
    "        target_cols=peptide_tasks[task][\"target_cols\"],\n",
    "        classification=True,\n",
    "        write=True,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MolMapNet Dataset Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "molmapnet_regression = {\n",
    "    \"Malaria\": {\n",
    "        \"smiles_col\": \"smiles\",\n",
    "        \"target_cols\": [\"activity\"],\n",
    "    },\n",
    "    \"LMC_H\": {\n",
    "        \"smiles_col\": \"Canonical_Smiles\",\n",
    "        \"target_cols\": [\"hlm_clearance[mL.min-1.g-1]\"],\n",
    "    },\n",
    "    \"LMC_R\": {\n",
    "        \"smiles_col\": \"Canonical_Smiles\",\n",
    "        \"target_cols\": [\"rlm_clearance[mL.min-1.g-1]\"],\n",
    "    },\n",
    "    \"LMC_M\": {\n",
    "        \"smiles_col\": \"Canonical_Smiles\",\n",
    "        \"target_cols\": [\"mlm_clearance[mL.min-1.g-1]\"],\n",
    "    },\n",
    "}\n",
    "molmapnet_classification = {\n",
    "    \"CYP450\": {\n",
    "        \"smiles_col\": \"smiles\",\n",
    "        \"target_cols\": [\n",
    "            \"label_1a2\",\n",
    "            \"label_2c9\",\n",
    "            \"label_2c19\",\n",
    "            \"label_2d6\",\n",
    "            \"label_3a4\",\n",
    "        ],\n",
    "    },\n",
    "    \"BACE_ChEMBL\": {\"smiles_col\": \"smiles\", \"target_cols\": [\"Class\"]},\n",
    "    \"BACE_NOVEL\": {\"smiles_col\": \"smiles\", \"target_cols\": [\"Class\"]},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in tqdm(\n",
    "    molmapnet_regression.keys(),\n",
    "    desc=\"Processing MolMapNet Classification Tasks\",\n",
    "):\n",
    "    if task == \"LMC_H\" or task == \"LMC_R\" or task == \"LMC_M\":\n",
    "        data_path = \"./data/raw/molmapnet/LMC/LMC.csv.gz\"\n",
    "    else:\n",
    "        data_path = f\"./data/raw/molmapnet/{task}/{task}.csv.gz\"\n",
    "    _ = preprocess_df(\n",
    "        task=task,\n",
    "        data_path=data_path,\n",
    "        smiles_col=molmapnet_regression[task][\"smiles_col\"],\n",
    "        target_cols=molmapnet_regression[task][\"target_cols\"],\n",
    "        classification=False,\n",
    "        write=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in tqdm(\n",
    "    molmapnet_classification.keys(),\n",
    "    desc=\"Processing MolMapNet Classification Tasks\",\n",
    "):\n",
    "    _ = preprocess_df(\n",
    "        task=task,\n",
    "        data_path=f\"./data/raw/molmapnet/{task}/{task}.csv.gz\",\n",
    "        smiles_col=molmapnet_classification[task][\"smiles_col\"],\n",
    "        target_cols=molmapnet_classification[task][\"target_cols\"],\n",
    "        classification=True,\n",
    "        write=True,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell Line Dataset Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_lines = [\n",
    "    \"A2780\",\n",
    "    \"CCRF-CEM\",\n",
    "    \"DU-145\",\n",
    "    \"HCT-15\",\n",
    "    \"KB\",\n",
    "    \"LoVo\",\n",
    "    \"PC-3\",\n",
    "    \"SK-OV-3\",\n",
    "]  # Cell lines to process\n",
    "\n",
    "for cell in tqdm(cell_lines, desc=\"Processing Cancer Cell Lines\"):\n",
    "    suppl = SDMolSupplier(\n",
    "        f\"./data/raw/kekulescope/cell-lines/{cell}/{cell}.sdf\"\n",
    "    )  # Read in SDF file\n",
    "    mols = [x for x in suppl if x is not None]  # Filter out None values\n",
    "    smiles = [MolToSmiles(x) for x in mols]  # Get SMILES\n",
    "    inhibition = [float(m.GetProp(\"pIC50\")) for m in mols]  # Get pIC50 values\n",
    "    df = pd.DataFrame({\"SMILES\": smiles, \"Target\": inhibition})  # Create DataFrame\n",
    "    df[\"SMILES\"] = df[\"SMILES\"].parallel_apply(std_smiles)  # Standardize SMILES\n",
    "    df.dropna(inplace=True)  # Drop NaN values\n",
    "    df.drop_duplicates(subset=[\"SMILES\"], inplace=True)  # Drop duplicates\n",
    "    write_path = Path(f\"./data/processed/tasks/{cell}\")  # Create write path\n",
    "    write_path.mkdir(parents=True, exist_ok=True)  # Create directory\n",
    "    df.to_parquet(write_path / f\"{cell}.parquet\", index=False)  # Write DataFrame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_df = create_summary(\"./data/processed/tasks/\")  # Create summary DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_df.head()  # Show tasks DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(task_df)  # Number of tasks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df: pd.DataFrame, split_type: str, fold_idx: int) -> dict:\n",
    "    \"\"\"Splits a dataset into train, validation and test sets.\n",
    "\n",
    "    :param df: DataFrame to split\n",
    "    :param root: Root directory\n",
    "    :param split_type: Split type\n",
    "    :param fold_idx: Fold index\n",
    "    :raises ValueError: Invalid split type\n",
    "    :return: Dictionary containing train, validation and test sets\n",
    "    \"\"\"\n",
    "    if split_type == \"random\":\n",
    "        splits = create_fold(\n",
    "            df=df, fold_seed=fold_idx, frac=(0.8, 0.1, 0.1)\n",
    "        )  # Create random split\n",
    "    elif split_type == \"scaffold\":\n",
    "        splits = create_scaffold_split(\n",
    "            df=df, seed=fold_idx, frac=(0.8, 0.1, 0.1), entity=\"SMILES\"\n",
    "        )  # Create scaffold split\n",
    "    elif split_type == \"butina\":\n",
    "        splits = create_butina_split(\n",
    "            df=df, seed=fold_idx, frac=(0.8, 0.1, 0.1), entity=\"SMILES\"\n",
    "        )  # Create butina split\n",
    "    else:\n",
    "        raise ValueError(\"Invalid split type\")\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descriptors(smi: str) -> np.ndarray:\n",
    "    \"\"\"Get descriptors from SMILES string.\n",
    "\n",
    "    :param smi: SMILES string\n",
    "    :return: Descriptor vector\n",
    "    \"\"\"\n",
    "    mol = MolFromSmiles(smi)  # Get molecule from SMILES string\n",
    "\n",
    "    # Get descriptors\n",
    "    desc_list = []\n",
    "    for _, func in Descriptors._descList:\n",
    "        try:\n",
    "            desc_list.append(func(mol))\n",
    "        except BaseException:\n",
    "            desc_list.append(0)\n",
    "    descriptors = np.asarray(desc_list)\n",
    "    descriptors = np.nan_to_num(\n",
    "        descriptors, nan=0.0, posinf=0.0, neginf=0.0\n",
    "    )  # Replace NaNs with 0\n",
    "    descriptors = descriptors / np.linalg.norm(descriptors)  # Normalize\n",
    "    return descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_as_hdf5(\n",
    "    root: Path,\n",
    "    task: str,\n",
    "    split_types: List[str],\n",
    "    fgroups_list: List[MolFromSmarts],\n",
    "    tokenizer: Tokenizer,\n",
    ") -> None:\n",
    "    \"\"\"Stores a dataset as an HDF5 file.\n",
    "\n",
    "    :param root: Root directory\n",
    "    :param task: Task name\n",
    "    :param split_types: Split types\n",
    "    :param fgroups_list: List of functional groups\n",
    "    :param tokenizer: Tokenizer\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(root / \"tasks\" / task / f\"{task}.parquet\")  # Read in DataFrame\n",
    "    labels = df.drop(columns=[\"SMILES\"]).values\n",
    "    df[\"fg\"] = df[\"SMILES\"].parallel_apply(lambda x: smiles2vector_fg(x, fgroups_list))\n",
    "    df[\"mfg\"] = df[\"SMILES\"].parallel_apply(lambda x: smiles2vector_mfg(x, tokenizer))\n",
    "    df[\"desc\"] = df[\"SMILES\"].parallel_apply(lambda x: get_descriptors(x))\n",
    "\n",
    "    with h5py.File(root / \"tasks\" / task / f\"{task}.hdf5\", \"w\") as f:\n",
    "        for split_type in split_types:\n",
    "            for i in range(5):\n",
    "                splits = split_dataset(df, split_type, i)\n",
    "                for split in [\"train\", \"valid\", \"test\"]:\n",
    "                    indices = df.loc[df[\"SMILES\"].isin(splits[split][\"SMILES\"])].index.tolist()\n",
    "                    for index in indices:\n",
    "                        smile, fg, mfg, desc = df.loc[index, [\"SMILES\", \"fg\", \"mfg\", \"desc\"]]\n",
    "                        label = labels[index]\n",
    "                        group_id = f\"{split_type}/fold_{i}/{split}/{smile}\"\n",
    "                        group = f.require_group(group_id)\n",
    "                        for key, value in (\n",
    "                            (\"fg\", fg),\n",
    "                            (\"mfg\", mfg),\n",
    "                            (\"desc\", desc),\n",
    "                            (\"label\", label),\n",
    "                        ):\n",
    "                            group.create_dataset(\n",
    "                                key, data=value, dtype=\"float32\", compression=\"lzf\"\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(\"./data/processed\")  # Root directory\n",
    "tokenize_dataset = \"pubchem\"\n",
    "frequency = 500\n",
    "\n",
    "# Get functional groups\n",
    "fgroups = pd.read_parquet(os.path.join(root, \"training\", \"fg.parquet\"))[\"SMARTS\"].tolist()\n",
    "fgroups_list = [MolFromSmarts(x) for x in fgroups]\n",
    "\n",
    "# Get tokenizer\n",
    "tokenizer = Tokenizer.from_file(\n",
    "    os.path.join(\n",
    "        root,\n",
    "        \"training\",\n",
    "        \"tokenizers\",\n",
    "        f\"BPE_{tokenize_dataset}_{frequency}.json\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in trange(len(task_df), desc=\"Processing Splits\"):\n",
    "    task = task_df.loc[i, \"Task\"]\n",
    "    datapoints = task_df.loc[i, \"Datapoints\"]\n",
    "    if datapoints <= 300000:  # type: ignore\n",
    "        split_types = [\"random\", \"scaffold\", \"butina\"]\n",
    "    else:\n",
    "        split_types = [\"random\"]\n",
    "    store_as_hdf5(root, task, split_types, fgroups_list, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
