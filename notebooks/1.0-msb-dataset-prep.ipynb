{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import modin.pandas as pd\n",
    "from rdkit import RDLogger\n",
    "from rdkit.Chem.rdmolfiles import MolToSmiles, SDMolSupplier\n",
    "from tdc.utils import create_fold, create_scaffold_split\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from src.data.components.utils import create_butina_split, std_smarts, std_smiles\n",
    "\n",
    "RDLogger.DisableLog(\"rdApp.*\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(\n",
    "    task: str,\n",
    "    data_path: str,\n",
    "    smiles_col: str,\n",
    "    target_cols: List[str],\n",
    "    classification: bool,\n",
    "    write: bool,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Preprocesses a dataframe for a given task.\n",
    "\n",
    "    Args:\n",
    "        task (str): Task name\n",
    "        data_path (str): Path to the raw dataframe\n",
    "        smiles_col (str): SMILES column\n",
    "        target_cols (List[str]): Target columns\n",
    "        classification (bool): Whether the task is classification or not\n",
    "        write (bool): Whether to write the preprocessed dataframe to disk\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Preprocessed dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(data_path)  # read dataframe\n",
    "    if classification:\n",
    "        df.fillna(0, inplace=True)  # fill nulls with 0\n",
    "    if target_cols == [\"multi_target\"]:\n",
    "        target_cols = df.columns[1:].to_list()  # select all columns except the first\n",
    "    df = df.loc[:, [smiles_col] + target_cols]  # select SMILES and target columns\n",
    "    df[\"SMILES\"] = df[smiles_col].apply(std_smiles)  # standardize SMILES\n",
    "    df.drop(columns=[smiles_col], inplace=True)  # drop original SMILES column\n",
    "    df.dropna(inplace=True)  # drop nulls\n",
    "    df.drop_duplicates(subset=[\"SMILES\"], inplace=True)  # drop duplicates\n",
    "    df.reset_index(drop=True, inplace=True)  # reset index\n",
    "    if write:\n",
    "        write_path = Path(f\"./data/processed/tasks/{task}\")\n",
    "        write_path.mkdir(parents=True, exist_ok=True)  # create directory if it doesn't exist\n",
    "        df.to_parquet(write_path / f\"{task}.parquet\", index=False)  # write to disk\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_summary(data_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Creates a summary dataframe for the dataset.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to the datasets\n",
    "    \"\"\"\n",
    "    num_datapoints = []  # number of datapoints per task\n",
    "    num_tasks = []  # number of tasks per task\n",
    "    tasks = []  # task names\n",
    "    for task_path in Path(data_path).iterdir():  # iterate over tasks\n",
    "        if not task_path.is_dir():  # skip files\n",
    "            continue\n",
    "        task = task_path.name  # get task name\n",
    "        df = pd.read_parquet(task_path / f\"{task}.parquet\")  # read dataframe\n",
    "        num_datapoints.append(df.shape[0])  # append number of datapoints\n",
    "        num_tasks.append(df.shape[1] - 1)  # append number of tasks\n",
    "        tasks.append(task)  # append task name\n",
    "    df = (\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"Task\": tasks,\n",
    "                \"Datapoints\": num_datapoints,\n",
    "                \"Num_tasks\": num_tasks,\n",
    "            }\n",
    "        )\n",
    "        .sort_values(\"Task\")\n",
    "        .reset_index(drop=True)  # create summary dataframe\n",
    "    )\n",
    "    df.to_parquet(Path(data_path) / \"summary.parquet\", index=False)  # write to disk\n",
    "    return df  # return summary dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional Groups Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/raw/training/fg.csv\")  # read dataframe\n",
    "drop_values = [\n",
    "    \"NOT [!#1!#6!#7!#8!#16!#15!#9!#17!#35!#53] AND [#6] >0\",\n",
    "    \"[!#1;!#6;!#7;!#8;!#9;!#11;!#12;!#15;!#16;!#17;!#19;!#20;!#35;!#53]\",\n",
    "    \"NOT [!#6!#1!#7!#8!#9!#17!#35!#53!#15!#14!#16]\",\n",
    "    \"[!#6!#1!#7!#8!#9!#17!#35!#53!#15!#14!#16]\",\n",
    "    \"[!#1!#6!#7!#8!#16!#15!#9!#17!#35!#53]\",\n",
    "    \"C(F)(F)C(F)(F)\",\n",
    "    \"(H[Cl,Br,I,F]).([*])\",\n",
    "    \"[!#1!#2!#5!#6!#7!#8!#9!#10!#14!#15!#16!#17!#18!#32!#33!#34!#35!#36!#51!#52!#53!#54!#84!#85!#86]~[#6,#7,#8,#16]\",\n",
    "    \"([*]).([*])\",\n",
    "    \"[#6] >0\",\n",
    "    \"[N;!R][C;!R](=O)[!#7] OR [N;!R][C;!R](=O)[N;!R]\",\n",
    "    \"[#16;X3v3+0]\",\n",
    "    \"N[F,Cl,Br,I]\",\n",
    "    \"[+,-]\",\n",
    "    \"([*][*][*]).([*][*][*])\",\n",
    "    \"[+]\",\n",
    "    \"[-]\",\n",
    "    \"[ERROR]\",\n",
    "]  # SMARTS to drop\n",
    "df = df[~df[\"SMARTS\"].isin(drop_values)].reset_index(drop=True)  # drop SMARTS\n",
    "df[\"SMARTS\"] = df[\"SMARTS\"].apply(std_smarts)  # standardize SMARTS\n",
    "df.dropna(inplace=True)  # drop nulls\n",
    "df.drop_duplicates(subset=[\"SMARTS\"], inplace=True)  # drop duplicates\n",
    "df.to_parquet(\"./data/processed/training/fg\", index=False)  # write to disk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Therapeutic Data Commons Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_tdc = [\n",
    "    \"herg_karim\",\n",
    "    \"herg\",\n",
    "    \"herg_central_inhib\",\n",
    "    \"dili\",\n",
    "    \"skin_reaction\",\n",
    "    \"ames\",\n",
    "    \"carcinogens_lagunin\",\n",
    "    \"sarscov2_3clpro_diamond\",\n",
    "    \"sarscov2_vitro_touret\",\n",
    "    \"orexin1_receptor_butkiewicz\",\n",
    "    \"m1_muscarinic_receptor_agonists_butkiewicz\",\n",
    "    \"m1_muscarinic_receptor_antagonists_butkiewicz\",\n",
    "    \"potassium_ion_channel_kir2.1_butkiewicz\",\n",
    "    \"kcnq2_potassium_channel_butkiewicz\",\n",
    "    \"cav3_t-type_calcium_channels_butkiewicz\",\n",
    "    \"choline_transporter_butkiewicz\",\n",
    "    \"serine_threonine_kinase_33_butkiewicz\",\n",
    "    \"tyrosyl-dna_phosphodiesterase_butkiewicz\",\n",
    "    \"pampa_ncats\",\n",
    "    \"hia_hou\",\n",
    "    \"pgp_broccatelli\",\n",
    "    \"bioavailability_ma\",\n",
    "    \"cyp2c9_substrate_carbonmangels\",\n",
    "    \"cyp2d6_substrate_carbonmangels\",\n",
    "    \"cyp3a4_substrate_carbonmangels\",\n",
    "]\n",
    "\n",
    "regression_tdc = [\n",
    "    \"ld50_zhu\",\n",
    "    \"herg_central_1uM\",\n",
    "    \"herg_central_10uM\",\n",
    "    \"caco2_wang\",\n",
    "    \"solubility_aqsoldb\",\n",
    "    \"ppbr_az\",\n",
    "    \"vdss_lombardo\",\n",
    "    \"half_life_obach\",\n",
    "    \"clearance_hepatocyte_az\",\n",
    "    \"clearance_microsome_az\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in tqdm(classification_tdc, desc=\"Processing TDC Classification datasets\"):\n",
    "    _ = preprocess_df(\n",
    "        task=task,\n",
    "        data_path=f\"./data/raw/tdc_data/{task}.csv.gz\",\n",
    "        smiles_col=\"Drug\",\n",
    "        target_cols=[\"Y\"],\n",
    "        classification=True,\n",
    "        write=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in tqdm(regression_tdc, desc=\"Processing TDC Regression datasets\"):\n",
    "    _ = preprocess_df(\n",
    "        task=task,\n",
    "        data_path=f\"./data/raw/tdc_data/{task}.csv.gz\",\n",
    "        smiles_col=\"Drug\",\n",
    "        target_cols=[\"Y\"],\n",
    "        classification=False,\n",
    "        write=True,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MoleculeNet Prep"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_tasks = {\n",
    "    \"Lipop\": [\"exp\"],\n",
    "    \"ESOL\": [\"measured log solubility in mols per litre\"],\n",
    "    \"FreeSolv\": [\"expt\"],\n",
    "    \"PDBbind-full\": [\"-logKd/Ki\"],\n",
    "    \"PDBbind-core\": [\"-logKd/Ki\"],\n",
    "    \"PDBbind-refined\": [\"-logKd/Ki\"],\n",
    "    \"qm7\": [\"u0_atom\"],\n",
    "    \"qm8\": [\n",
    "        \"E1-CC2\",\n",
    "        \"E2-CC2\",\n",
    "        \"f1-CC2\",\n",
    "        \"f2-CC2\",\n",
    "        \"E1-PBE0\",\n",
    "        \"E2-PBE0\",\n",
    "        \"f1-PBE0\",\n",
    "        \"f2-PBE0\",\n",
    "        \"E1-CAM\",\n",
    "        \"E2-CAM\",\n",
    "        \"f1-CAM\",\n",
    "        \"f2-CAM\",\n",
    "    ],\n",
    "    \"qm9\": [\n",
    "        \"mu\",\n",
    "        \"alpha\",\n",
    "        \"homo\",\n",
    "        \"lumo\",\n",
    "        \"gap\",\n",
    "        \"r2\",\n",
    "        \"zpve\",\n",
    "        \"cv\",\n",
    "        \"u0\",\n",
    "        \"u298\",\n",
    "        \"h298\",\n",
    "        \"g298\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in tqdm(regression_tasks.keys(), desc=\"Processing Regression Tasks\"):\n",
    "    _ = preprocess_df(\n",
    "        task=task,\n",
    "        data_path=f\"./data/raw/moleculenet/{task}/{task}.csv.gz\",\n",
    "        smiles_col=\"smiles\",\n",
    "        target_cols=regression_tasks[task],\n",
    "        classification=False,\n",
    "        write=True,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_tasks = {\n",
    "    \"ToxCast\": [\"multi_target\"],\n",
    "    \"BACE\": [\"Class\"],\n",
    "    \"ChEMBL\": [\"multi_target\"],\n",
    "    \"PCBA\": [\"multi_target\"],\n",
    "    \"BBBP\": [\"p_np\"],\n",
    "    \"HIV\": [\"HIV_active\"],\n",
    "    \"MUV\": [\n",
    "        \"MUV-466\",\n",
    "        \"MUV-548\",\n",
    "        \"MUV-600\",\n",
    "        \"MUV-644\",\n",
    "        \"MUV-652\",\n",
    "        \"MUV-689\",\n",
    "        \"MUV-692\",\n",
    "        \"MUV-712\",\n",
    "        \"MUV-713\",\n",
    "        \"MUV-733\",\n",
    "        \"MUV-737\",\n",
    "        \"MUV-810\",\n",
    "        \"MUV-832\",\n",
    "        \"MUV-846\",\n",
    "        \"MUV-852\",\n",
    "        \"MUV-858\",\n",
    "        \"MUV-859\",\n",
    "    ],\n",
    "    \"Tox21\": [\n",
    "        \"NR-AR\",\n",
    "        \"NR-AR-LBD\",\n",
    "        \"NR-AhR\",\n",
    "        \"NR-Aromatase\",\n",
    "        \"NR-ER\",\n",
    "        \"NR-ER-LBD\",\n",
    "        \"NR-PPAR-gamma\",\n",
    "        \"SR-ARE\",\n",
    "        \"SR-ATAD5\",\n",
    "        \"SR-HSE\",\n",
    "        \"SR-MMP\",\n",
    "        \"SR-p53\",\n",
    "    ],\n",
    "    \"SIDER\": [\"multi_target\"],\n",
    "    \"ClinTox\": [\"FDA_APPROVED\", \"CT_TOX\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in tqdm(classification_tasks.keys(), desc=\"Processing Classification Tasks\"):\n",
    "    _ = preprocess_df(\n",
    "        task=task,\n",
    "        data_path=f\"./data/raw/moleculenet/{task}/{task}.csv.gz\",\n",
    "        smiles_col=\"smiles\",\n",
    "        target_cols=classification_tasks[task],\n",
    "        classification=True,\n",
    "        write=True,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peptide Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peptide_tasks = {\n",
    "    \"1625_aa\": {\"smiles_col\": \"Peptide_smiles\", \"target_cols\": [\"cleavage\"]},\n",
    "    \"746_aa\": {\"smiles_col\": \"Peptide_smiles\", \"target_cols\": [\"cleavage\"]},\n",
    "    \"Impens\": {\"smiles_col\": \"Peptide_smiles\", \"target_cols\": [\"cleavage\"]},\n",
    "    \"Schilling\": {\"smiles_col\": \"Peptide_smiles\", \"target_cols\": [\"cleavage\"]},\n",
    "    \"Ecoli\": {\"smiles_col\": \"smiles\", \"target_cols\": [\"Encoded_Activity\"]},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in tqdm(peptide_tasks.keys(), desc=\"Processing Peptide Tasks\"):\n",
    "    _ = preprocess_df(\n",
    "        task=task,\n",
    "        data_path=f\"./data/raw/peptide_data/{task}.csv.gz\",\n",
    "        smiles_col=peptide_tasks[task][\"smiles_col\"],\n",
    "        target_cols=peptide_tasks[task][\"target_cols\"],\n",
    "        classification=True,\n",
    "        write=True,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MolMapNet Dataset Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "molmapnet_regression = {\n",
    "    \"Malaria\": {\n",
    "        \"smiles_col\": \"smiles\",\n",
    "        \"target_cols\": [\"activity\"],\n",
    "    },\n",
    "    \"LMC_H\": {\n",
    "        \"smiles_col\": \"Canonical_Smiles\",\n",
    "        \"target_cols\": [\"hlm_clearance[mL.min-1.g-1]\"],\n",
    "    },\n",
    "    \"LMC_R\": {\n",
    "        \"smiles_col\": \"Canonical_Smiles\",\n",
    "        \"target_cols\": [\"rlm_clearance[mL.min-1.g-1]\"],\n",
    "    },\n",
    "    \"LMC_M\": {\n",
    "        \"smiles_col\": \"Canonical_Smiles\",\n",
    "        \"target_cols\": [\"mlm_clearance[mL.min-1.g-1]\"],\n",
    "    },\n",
    "}\n",
    "molmapnet_classification = {\n",
    "    \"CYP450\": {\n",
    "        \"smiles_col\": \"smiles\",\n",
    "        \"target_cols\": [\n",
    "            \"label_1a2\",\n",
    "            \"label_2c9\",\n",
    "            \"label_2c19\",\n",
    "            \"label_2d6\",\n",
    "            \"label_3a4\",\n",
    "        ],\n",
    "    },\n",
    "    \"BACE_ChEMBL\": {\"smiles_col\": \"smiles\", \"target_cols\": [\"Class\"]},\n",
    "    \"BACE_NOVEL\": {\"smiles_col\": \"smiles\", \"target_cols\": [\"Class\"]},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in tqdm(\n",
    "    molmapnet_regression.keys(),\n",
    "    desc=\"Processing MolMapNet Classification Tasks\",\n",
    "):\n",
    "    if task == \"LMC_H\" or task == \"LMC_R\" or task == \"LMC_M\":\n",
    "        data_path = \"./data/raw/molmapnet/LMC/LMC.csv.gz\"\n",
    "    else:\n",
    "        data_path = f\"./data/raw/molmapnet/{task}/{task}.csv.gz\"\n",
    "    _ = preprocess_df(\n",
    "        task=task,\n",
    "        data_path=data_path,\n",
    "        smiles_col=molmapnet_regression[task][\"smiles_col\"],\n",
    "        target_cols=molmapnet_regression[task][\"target_cols\"],\n",
    "        classification=False,\n",
    "        write=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in tqdm(\n",
    "    molmapnet_classification.keys(),\n",
    "    desc=\"Processing MolMapNet Classification Tasks\",\n",
    "):\n",
    "    _ = preprocess_df(\n",
    "        task=task,\n",
    "        data_path=f\"./data/raw/molmapnet/{task}/{task}.csv.gz\",\n",
    "        smiles_col=molmapnet_classification[task][\"smiles_col\"],\n",
    "        target_cols=molmapnet_classification[task][\"target_cols\"],\n",
    "        classification=True,\n",
    "        write=True,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell Line Dataset Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_lines = [\n",
    "    \"A2780\",\n",
    "    \"CCRF-CEM\",\n",
    "    \"DU-145\",\n",
    "    \"HCT-15\",\n",
    "    \"KB\",\n",
    "    \"LoVo\",\n",
    "    \"PC-3\",\n",
    "    \"SK-OV-3\",\n",
    "]  # Cell lines to process\n",
    "\n",
    "for cell in tqdm(cell_lines, desc=\"Processing Cancer Cell Lines\"):\n",
    "    suppl = SDMolSupplier(\n",
    "        f\"./data/raw/kekulescope/cell-lines/{cell}/{cell}.sdf\"\n",
    "    )  # Read in SDF file\n",
    "    mols = [x for x in suppl if x is not None]  # Filter out None values\n",
    "    smiles = [MolToSmiles(x) for x in mols]  # Get SMILES\n",
    "    inhibition = [float(m.GetProp(\"pIC50\")) for m in mols]  # Get pIC50 values\n",
    "    df = pd.DataFrame({\"SMILES\": smiles, \"Target\": inhibition})  # Create DataFrame\n",
    "    df[\"SMILES\"] = df[\"SMILES\"].apply(std_smiles)  # Standardize SMILES\n",
    "    df.dropna(inplace=True)  # Drop NaN values\n",
    "    df.drop_duplicates(subset=[\"SMILES\"], inplace=True)  # Drop duplicates\n",
    "    write_path = Path(f\"./data/processed/tasks/{cell}\")  # Create write path\n",
    "    write_path.mkdir(parents=True, exist_ok=True)  # Create directory\n",
    "    df.to_parquet(write_path / f\"{cell}.parquet\", index=False)  # Write DataFrame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_df = create_summary(\"./data/processed/tasks/\")  # Create summary DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_df.head()  # Show tasks DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(task_df)  # Number of tasks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(task: str, root: Path, split_type: str, num_folds: int) -> None:\n",
    "    \"\"\"Split dataset into train, validation, and test sets.\n",
    "\n",
    "    Args:\n",
    "        task (str): Dataset name\n",
    "        root (Path): Root directory\n",
    "        split_type (str): Split type. Either \"random\" or \"scaffold\"\n",
    "        num_folds (int): Number of folds\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Invalid split type\n",
    "    \"\"\"\n",
    "    import gc\n",
    "\n",
    "    df = pd.read_parquet(root / task / f\"{task}.parquet\")  # Read in DataFrame\n",
    "\n",
    "    for i in range(num_folds):\n",
    "        write_path = Path(root / task / f\"splits/{split_type}/fold_{i}\")  # Create write path\n",
    "        write_path.mkdir(parents=True, exist_ok=True)  # create directory if it doesn't exist\n",
    "        if split_type == \"random\":\n",
    "            splits = create_fold(df=df, fold_seed=i, frac=(0.8, 0.1, 0.1))  # Create random split\n",
    "        elif split_type == \"scaffold\":\n",
    "            splits = create_scaffold_split(\n",
    "                df=df, seed=i, frac=(0.8, 0.1, 0.1), entity=\"SMILES\"\n",
    "            )  # Create scaffold split\n",
    "        elif split_type == \"butina\":\n",
    "            splits = create_butina_split(\n",
    "                df=df, seed=i, frac=(0.8, 0.1, 0.1), entity=\"SMILES\"\n",
    "            )  # Create butina split\n",
    "        else:\n",
    "            raise ValueError(\"Invalid split type\")\n",
    "\n",
    "        splits[\"train\"].to_parquet(write_path / \"train.parquet\", index=False)  # Write train split\n",
    "        splits[\"valid\"].to_parquet(\n",
    "            write_path / \"val.parquet\", index=False\n",
    "        )  # Write validation split\n",
    "        splits[\"test\"].to_parquet(write_path / \"test.parquet\", index=False)  # Write test split\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(\"./data/processed/tasks/\")\n",
    "for i in trange(len(task_df), desc=\"Processing Splits\"):\n",
    "    task = task_df.loc[i, \"Task\"]\n",
    "    datapoints = task_df.loc[i, \"Datapoints\"]\n",
    "    if datapoints <= 200000:\n",
    "        split_types = [\"random\", \"scaffold\", \"butina\"]\n",
    "    else:\n",
    "        split_types = [\"random\", \"scaffold\"]\n",
    "    for split_type in split_types:\n",
    "        split_dataset(task, root, split_type, 5)  # Create splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
