_target_: src.models.fgr_module.FGRLitModule

base_optimizer:
  _target_: torch.optim.AdamW
  _partial_: true

optimizer:
  _target_: src.models.components.losses.SAM
  _partial_: true
  lr: 0.001
  weight_decay: 0.001
  rho: 0.05
  adaptive: true

scheduler:
  _target_: torch.optim.lr_scheduler.OneCycleLR
  _partial_: true
  max_lr: 1.0e-2

net:
  _target_: src.models.components.autoencoder.FGRModel
  fg_input_dim: 2672
  num_feat_dim: 211
  method: "${data.method}"
  tokenize_dataset: "${data.tokenize_dataset}"
  frequency: "${data.frequency}"
  dataset: "${data.dataset}"
  descriptors: "${data.descriptors}"
  hidden_dim1: 2048
  hidden_dim2: 1024
  hidden_dim3: 512
  bottleneck_dim: 256
  output_dim1: 256
  output_dim2: 128
  output_dim3: 64
  dropout: 0
  activation: "selu"
  tie_weights: False

recon_loss:
  _target_: src.models.components.losses.FocalLoss
  alpha: 1.0
  gamma: 0

loss_weights:
  recon_loss: 1.0
  ubc_loss: 0.0

# compile model for faster training with pytorch 2.0
compile: false
